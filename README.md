# Cancer Classification Model ğŸš€

## Project Overview ğŸ”¬

This repository contains machine learning models designed for cancer classification using various algorithms including Logistic Regression, K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Random Forest, AdaBoost, Bagging, XGBoost, and Stacking. The models analyze various features from a cancer dataset to classify samples as malignant or benign.

## Setup and Installation âš™ï¸

### Prerequisites
- Python 3.8+
- pandas
- numpy
- scikit-learn
- matplotlib
- seaborn
- imbalanced-learn (for SMOTE)

### Installation
```bash
git clone https://github.com/[username]/Cancer_Classification_Model.git
cd Cancer_Classification_Model
pip install -r requirements.txt
```

## Data Description ğŸ“Š

The dataset contains features extracted from breast cancer cell nuclei, including:
- Radius
- Texture
- Perimeter
- Area
- Smoothness
- And other characteristics

Each sample is classified as malignant (1) or benign (0).

## Directory Structure ğŸ“

```
Cancer_Classification_Model/
â”œâ”€â”€ EDA.ipynb                  # Exploratory Data Analysis
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ Logistic/                  # Logistic Regression implementation
â”‚   â””â”€â”€ Model.ipynb
â”œâ”€â”€ KNN/                       # K-Nearest Neighbors implementation  
â”‚   â””â”€â”€ Model.ipynb
â”œâ”€â”€ SVM/                       # Support Vector Machine implementation
â”‚   â””â”€â”€ Model.ipynb
â”œâ”€â”€ RF/                        # Random Forest implementation
â”‚   â””â”€â”€ Model.ipynb
â”œâ”€â”€ ADA/                       # AdaBoost implementation
â”‚   â””â”€â”€ Model.ipynb
â”œâ”€â”€ Bagging/                   # Bagging implementation
â”‚   â””â”€â”€ Model.ipynb
â”œâ”€â”€ XGBoost/                   # XGBoost implementation
â”‚   â””â”€â”€ Model.ipynb
â””â”€â”€ Stacking/                  # Stacking implementation
    â””â”€â”€ Model.ipynb
```

## Models Implemented ğŸ¤–

### Logistic Regression ğŸ“ˆ
- Implementation of both L1 and L2 regularization
- Hyperparameter tuning via GridSearchCV
- Pipeline integration for workflow management

### K-Nearest Neighbors ğŸ§®
- Different distance metrics (Euclidean, Manhattan, Minkowski)
- Various weighting schemes (uniform, distance)
- Hyperparameter optimization for k value

### Support Vector Machine ğŸ”
- Linear and non-linear kernels
- Hyperparameter tuning for C and gamma parameters
- Class imbalance handling

### Random Forest ğŸŒ²
- Ensemble of decision trees
- Feature importance analysis
- Bootstrapping and random feature selection
- Hyperparameter optimization for number of trees and tree depth

### AdaBoost ğŸ”„
- Adaptive boosting algorithm
- Sequential learning of weak classifiers
- Weight adjustment for misclassified samples
- Hyperparameter tuning for learning rate and number of estimators

### Bagging ğŸ“¦
- Bootstrap aggregating technique
- Parallel ensemble of base classifiers
- Reduction of variance in the model
- Customizable base estimator selection

### XGBoost ğŸš€
- Gradient boosting implementation with regularization
- Early stopping to prevent overfitting
- Efficient handling of sparse data
- Advanced hyperparameter tuning

### Stacking ğŸ—ï¸
- Meta-ensemble learning approach
- Multiple base models with a meta-classifier
- Cross-validation for meta-model training
- Diverse base model selection for robust predictions

## Data Preprocessing ğŸ§¹

- Feature standardization/scaling
- Outlier detection and handling
- Checking for missing values
- Correlation analysis
- SMOTE for handling class imbalance

## Evaluation Metrics ğŸ“

The models are evaluated using:
- Accuracy
- Precision
- Recall
- F1-score
- Confusion Matrix

## Usage Instructions ğŸ“‹

### Exploratory Data Analysis
```python
# Run the EDA notebook
jupyter notebook EDA.ipynb
```

### Model Training and Evaluation
```python
# Run the respective model notebook
jupyter notebook [Model_Type]/Model.ipynb  # where Model_Type is one of: Logistic, KNN, SVM, RF, ADA, Bagging, XGBoost, Stacking
```

## Results and Comparison ğŸ†

Each model's performance is evaluated on both training and testing datasets to compare accuracy, precision, recall, and F1-scores. The evaluation helps determine the best model for cancer classification based on these metrics.

Key performance highlights:

- **KNN** ğŸ¥‡ achieved the highest performance with 98% accuracy and 100% precision without balancing the dataset, and maintained 98% accuracy with 98% precision after balancing using SMOTE
- **AdaBoost** ğŸ¥ˆ performed strongly with 97% accuracy and 97% precision
- **XGBoost** ğŸ¥‰ delivered robust results with 97% accuracy and 95% precision
- Other models showed varying performance levels across metrics, with ensemble methods generally outperforming individual classifiers

## Best Practices Used âœ…

- Cross-validation for robust model evaluation
- Pipeline architecture to prevent data leakage
- Hyperparameter optimization using GridSearchCV
- Proper train-test splitting
- Handling class imbalance with SMOTE

## Future Improvements ğŸ”®

- Deep learning approaches

## Source ğŸ“‘
[Cancer Dataset](https://www.kaggle.com/datasets/erdemtaha/cancer-data)


